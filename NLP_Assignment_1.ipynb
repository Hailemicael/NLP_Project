{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_nCKuybUvsr",
        "outputId": "fa67cc72-1d29-4efd-8896-a1ef5507fbc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.7.22)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxcmMCkSUmGh",
        "outputId": "9dc883c2-b75e-463d-baa8-e6181a2bf307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "52  cuisine style cooking characterized distinctiv...  non-medical\n",
            "11  blood vessel component circulatory system tran...      medical\n",
            "40  pandemic pandemik epidemic infectious disease ...      medical\n",
            "8   oncology branch medicine deal study treatment ...      medical\n",
            "77  country distinct part world state nation polit...  non-medical\n",
            "..                                                ...          ...\n",
            "75  astronomy natural science study celestial obje...  non-medical\n",
            "32  healthy diet diet maintains improves overall h...      medical\n",
            "66  recipe set instruction describes prepare make ...  non-medical\n",
            "55  history derived ancient greek histora inquiry ...  non-medical\n",
            "65  gastronomy study relationship food culture art...  non-medical\n",
            "\n",
            "[82 rows x 2 columns]\n",
            "Dataset saved as 'medical_non_medical_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import wikipediaapi\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to fetch content from Wikipedia using wikipediaapi\n",
        "def fetch_content(title, wiki_wiki):\n",
        "    # Fetches content from Wikipedia for a given title.\n",
        "    page_py = wiki_wiki.page(title)\n",
        "    content = page_py.text\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(clean_text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = \" \".join(tokens)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# Set up wikipediaapi\n",
        "wiki_wiki = wikipediaapi.Wikipedia('NLP_Project(hailelulseged281913@gmail.com)', 'en')\n",
        "\n",
        "# List of medical and non-medical keywords\n",
        "medical_keywords = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Pharmacy\", \"Immunology\",\n",
        "                    \"Pathology\", \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\"vascular\",\n",
        "                    \"orthopedic\", \"dermatology\", \"endocrinology\", \"gastroenterology\", \"pulmonary\", \"neurosurgery\",\n",
        "                    \"ophthalmology\", \"urology\", \"radiology\", \"anesthesiology\", \"genetics\", \"oncologist\", \"hematology\",\n",
        "                    \"immunotherapy\", \"pediatrician\", \"psychiatry\", \"dentist\", \"Anatomy\",\n",
        "                    \"Physiology\", \"Biochemistry\",\"balanced diet\",\"Ailment\", \"Affliction\", \"Illness\", \"Sickness\", \"Acute\",\n",
        "                    \"Hereditary\", \"Infectious\", \"Pandemic\",\"nurse\",\"Doctor\",\"Alzheimer\",]\n",
        "non_medical_keywords = [\"Art\", \"Literature\", \"Philosophy\", \"Science\", \"Technology\", \"Space\",\n",
        "                        \"Environment\", \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\", \"History\", \"Ancient_Civilizations\",\n",
        "                        \"Archaeology\", \"painting\", \"sculpture\", \"literary\", \"fiction\", \"poetry\", \"philosopher\",\n",
        "                        \"culinary\", \"gastronomy\", \"recipe\", \"culinary\", \"history\", \"historical\", \"architectural\",\n",
        "                        \"archaeological\", \"civilization\",\"culture\", \"Engineering\", \"Astronomy\", \"Cosmology\",\"country\",\"Industry\",\"ocean\",\"charger\",\"battery\"]\n",
        "\n",
        "# Fetch content for medical keywords\n",
        "medical_content_list = [fetch_content(keyword, wiki_wiki) for keyword in medical_keywords]\n",
        "\n",
        "# Fetch content for non-medical keywords\n",
        "non_medical_content_list = [fetch_content(keyword, wiki_wiki) for keyword in non_medical_keywords]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content_list, \"label\": [\"medical\"] * len(medical_content_list)}\n",
        "non_medical_data = {\"text\": non_medical_content_list, \"label\": [\"non-medical\"] * len(non_medical_content_list)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes and shuffle rows\n",
        "df = pd.concat([df_medical, df_non_medical], ignore_index=True).sample(frac=1)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n",
        "df.to_csv('medical_non_medical_dataset.csv', index=False)\n",
        "print(\"Dataset saved as 'medical_non_medical_dataset.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('medical_non_medical_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=100)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Handle missing values in X_train\n",
        "X_train = X_train.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Resampling using SMOTE\n",
        "sampler = SMOTE(sampling_strategy='auto', k_neighbors=5)\n",
        "X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Selection and Training (Naive Bayes)\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Model Evaluation (Naive Bayes)\n",
        "nb_predictions = nb_model.predict(X_test_tfidf)\n",
        "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
        "print(f\"\\nNaive Bayes Accuracy: {nb_accuracy}\")\n",
        "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, nb_predictions))\n",
        "\n",
        "# Example of making predictions on more new data using Naive Bayes\n",
        "new_data_extended = [\n",
        "    \"heart disease\",\n",
        "    \"expression varies across cultures.\",\n",
        "    \"New year in Ethiopia is good.\",\n",
        "    \"my health is not good\",\n",
        "    \"doctors support patient\",\n",
        "    \"let us see my country.\",\n",
        "    \"The latest novel by a famous author.\",\n",
        "    \"Advancements in artificial intelligence.\",\n",
        "    \"Exploring outer space and galaxies.\",\n",
        "    \"The best recipe for chocolate cake.\",\n",
        "    \"Historical landmarks in ancient civilizations.\",\n",
        "    \"Innovations in technology and robotics.\",\n",
        "    \"Creating beautiful paintings and sculptures.\",\n",
        "    \"The impact of climate change on the environment.\",\n",
        "    \"The science of astronomy and celestial bodies.\"\n",
        "]\n",
        "\n",
        "new_data_extended_tfidf = tfidf_vectorizer.transform(new_data_extended)\n",
        "new_predictions_extended = nb_model.predict(new_data_extended_tfidf)\n",
        "\n",
        "print(\"\\nNaive Bayes Predictions on more new data:\")\n",
        "for text, prediction in zip(new_data_extended, new_predictions_extended):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUJjJxu0U7eo",
        "outputId": "7cdc6241-7147-4941-8942-1f7c6341bcd4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        44\n",
            "non-medical    38\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Naive Bayes Accuracy: 0.92\n",
            "Naive Bayes Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       1.00      0.88      0.93        16\n",
            " non-medical       0.82      1.00      0.90         9\n",
            "\n",
            "    accuracy                           0.92        25\n",
            "   macro avg       0.91      0.94      0.92        25\n",
            "weighted avg       0.93      0.92      0.92        25\n",
            "\n",
            "\n",
            "Naive Bayes Predictions on more new data:\n",
            "heart disease - Predicted: medical\n",
            "expression varies across cultures. - Predicted: non-medical\n",
            "New year in Ethiopia is good. - Predicted: non-medical\n",
            "my health is not good - Predicted: medical\n",
            "doctors support patient - Predicted: medical\n",
            "let us see my country. - Predicted: non-medical\n",
            "The latest novel by a famous author. - Predicted: non-medical\n",
            "Advancements in artificial intelligence. - Predicted: non-medical\n",
            "Exploring outer space and galaxies. - Predicted: non-medical\n",
            "The best recipe for chocolate cake. - Predicted: non-medical\n",
            "Historical landmarks in ancient civilizations. - Predicted: non-medical\n",
            "Innovations in technology and robotics. - Predicted: non-medical\n",
            "Creating beautiful paintings and sculptures. - Predicted: non-medical\n",
            "The impact of climate change on the environment. - Predicted: non-medical\n",
            "The science of astronomy and celestial bodies. - Predicted: non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('medical_non_medical_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=100)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Handle missing values in X_train\n",
        "X_train = X_train.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Resampling using SMOTE\n",
        "sampler = SMOTE(sampling_strategy='auto', k_neighbors=5)\n",
        "X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Selection and Training (Logistic Regression)\n",
        "model = LogisticRegression(random_state=100)\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on more new data using Naive Bayes\n",
        "new_data_extended = [\n",
        "    \"heart disease\",\n",
        "    \"expression varies across cultures.\",\n",
        "    \"New year in Ethiopia is good.\",\n",
        "    \"my health is not good\",\n",
        "    \"doctors support patient\",\n",
        "    \"let us see my country.\",\n",
        "    \"The latest novel by a famous author.\",\n",
        "    \"Advancements in artificial intelligence.\",\n",
        "    \"Exploring outer space and galaxies.\",\n",
        "    \"The best recipe for chocolate cake.\",\n",
        "    \"Historical landmarks in ancient civilizations.\",\n",
        "    \"Innovations in technology and robotics.\",\n",
        "    \"Creating beautiful paintings and sculptures.\",\n",
        "    \"The impact of climate change on the environment.\",\n",
        "    \"The science of astronomy and celestial bodies.\"\n",
        "]\n",
        "\n",
        "new_data_extended_tfidf = tfidf_vectorizer.transform(new_data_extended)\n",
        "new_predictions_extended = nb_model.predict(new_data_extended_tfidf)\n",
        "\n",
        "print(\"\\nNaive Bayes Predictions on more new data:\")\n",
        "for text, prediction in zip(new_data_extended, new_predictions_extended):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5rHi2_IaSPz",
        "outputId": "634dca35-efee-402c-afa4-caca400804b9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        44\n",
            "non-medical    38\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Accuracy: 0.92\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       1.00      0.88      0.93        16\n",
            " non-medical       0.82      1.00      0.90         9\n",
            "\n",
            "    accuracy                           0.92        25\n",
            "   macro avg       0.91      0.94      0.92        25\n",
            "weighted avg       0.93      0.92      0.92        25\n",
            "\n",
            "\n",
            "Naive Bayes Predictions on more new data:\n",
            "heart disease - Predicted: medical\n",
            "expression varies across cultures. - Predicted: non-medical\n",
            "New year in Ethiopia is good. - Predicted: non-medical\n",
            "my health is not good - Predicted: medical\n",
            "doctors support patient - Predicted: medical\n",
            "let us see my country. - Predicted: non-medical\n",
            "The latest novel by a famous author. - Predicted: non-medical\n",
            "Advancements in artificial intelligence. - Predicted: non-medical\n",
            "Exploring outer space and galaxies. - Predicted: non-medical\n",
            "The best recipe for chocolate cake. - Predicted: non-medical\n",
            "Historical landmarks in ancient civilizations. - Predicted: non-medical\n",
            "Innovations in technology and robotics. - Predicted: non-medical\n",
            "Creating beautiful paintings and sculptures. - Predicted: non-medical\n",
            "The impact of climate change on the environment. - Predicted: non-medical\n",
            "The science of astronomy and celestial bodies. - Predicted: non-medical\n"
          ]
        }
      ]
    }
  ]
}